{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data stored in Cloudant DB from Watson IoT Platform and Anomaly Detection by using IBM Watson Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Install the spark-sql-cloudant package (library for reading data from Cloudant database using Spark SQL) in your IBM Watson Studio account by executing the following cell, and then restart the kernel. You will use **pixiedust** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pixiedust\n",
    "import pixiedust\n",
    "# Use play-json version 2.5.9. Latest version is not supported at this time.\n",
    "pixiedust.installPackage(\"com.typesafe.play:play-json_2.11:2.5.9\")\n",
    "# Get the matching version sql-cloudant library\n",
    "pixiedust.installPackage(\"org.apache.bahir:spark-sql-cloudant_2.11:0\")\n",
    "# To fix PyJavaError\n",
    "pixiedust.packageManager.uninstallPackage(\"org.apache.bahir:spark-sql-cloudant_2.11:2.2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User input required** : Cloudant credentials.\n",
    "\n",
    "If you have a connection with Cloudant set up for this project, complete the following steps:\n",
    "1.\tImport your Cloudant credentials by clicking on the following cell below to select it\n",
    "2.\tClick **Find and Add Data** \n",
    "3.\tSelect the Connections tab and click on **Insert to code**. A dictionary called *credentials_1* is  added to the cell that contains the Cloudant credentials. If the dictionary has another name, change it to *credentials_1*. \n",
    "4.\tRun the cell. \n",
    "\n",
    "If you don’t have a connection with Cloudant set up, the credentials can be found on IBM Cloud dashboard by completing the following steps:\n",
    "\n",
    "1.\tGo to your Cloudant service on IBM Cloud, \n",
    "2.\tSelect its Service Credentials section on the left \n",
    "3.\tClick **View Credentials** to view the username and password. \n",
    "4.\tUpdate the *username* and *password* variables with Cloudant’s username and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This empty cell will be populated with your Cloudant credentials if you follow the steps explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = credentials_1[\"username\"]\n",
    "password = credentials_1[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = username + '.cloudant.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User input required**: Cloudant database name.\n",
    "\n",
    "If you are not sure which database contains the data that you want to import, go to your Cloudant service on IBM Cloud and click **Launch** to display the database name. Update the *dbName* variable with the database name, for example\n",
    " *iotp_abcdef_default_2018-01-10*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbName = \"DBName\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Cloudant database that is generated by WIoTP connector for historical data.\n",
    "\n",
    "The following code connects to Cloudant NoSQL DB and returns an RDD data frame for the stored IoT data. The line `option(\"jsonstore.rdd.partitions\", 4)` is needed only if your Cloudant service plan is *lite* because this plan has an access quota of 5 requests per second. \n",
    "Spark might run parallel jobs that might lead to more than 5 requests being made in one second. If this happens, a \"too many requests\" error is raised. To resolve this error, decrease the value for the *jsonstore.rdd.partitions* option to 2. For paid service plans this line can be commented out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cloudantdata=sqlContext.read.format(\"org.apache.bahir.cloudant\").\\\n",
    "option(\"cloudant.host\", host).\\\n",
    "option(\"cloudant.username\", username).\\\n",
    "option(\"cloudant.password\", password).\\\n",
    "option(\"view\",\"_design/iotp/_view/by-date\").\\\n",
    "option(\"jsonstore.rdd.partitions\", 4).\\\n",
    "load(dbName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudantdata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All IoT data is located under the value column.\n",
    "\n",
    "Next, transform this hierarchical data frame into a flat one, and convert the timestamp from string type into a timestamp type.\n",
    "The function withColumn adds a column named 'ts' to the data frame, and calculates it's content based on timestamp column (string), by using the to_ts function that was defined.\n",
    "The cache() function of a data frame caches the data frame in memory, this is very useful when data is accessed repeatedly.\n",
    "\n",
    "The *deviceData* is a temporary view in the Spark Session and can be used for select statements if you want to write raw SQL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import udf, col, asc, desc,to_date, unix_timestamp, weekofyear, countDistinct\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the string cell into a timestamp type:\n",
    "str_to_ts =  udf (lambda d: datetime.strptime(d, \"%Y-%m-%dT%H:%M:%S.%fZ\"), TimestampType())\n",
    "\n",
    "sparkDf = cloudantdata.selectExpr(\"value.deviceId as deviceId\", \"value.deviceType as deviceType\", \"value.eventType as eventType\" ,  \"value.timestamp as timestamp\", \"value.data.*\",\"value.data.d.oa as oa\",\"value.data.d.ob as ob\",\"value.data.d.og as og\")\n",
    "sparkDf = sparkDf.withColumn('ts', str_to_ts(col('timestamp')))\n",
    "sparkDf.cache()\n",
    "sparkDf.createOrReplaceTempView(\"deviceData\")\n",
    "\n",
    "# show the resulting schema and data \n",
    "sparkDf.printSchema()\n",
    "spark.sql(\"SELECT * from deviceData\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the values of deviceId and deviceType for which you want to see visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceId = 'phone'\n",
    "deviceType = 'simulator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with device movement data, you can also add acceleration data(ax,ay,az).\n",
    "\n",
    "## Data visualization and comprehension\n",
    "\n",
    "### Device Health \n",
    "\n",
    "In this section we will see how to learn about the population of IoT devices and answer questions such as: \n",
    "1. How many reports each device type had?\n",
    "2. What is the breakdown of the devices per device type?\n",
    "3. How many reports have been sent by each device? \n",
    "4. How many reports each event type had? \n",
    "5. How many devices reported in a given time interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Spark to prepare the data for visualization, because Spark can support big data processing. When the data is ready for visualization, convert Spark data Frame into Pandas data Frame, because Pandas has good visualization support.\n",
    "\n",
    "#### How many reports each device type had? \n",
    "\n",
    "Setting the *deviceType* as index of the created Pandas data frame causes the bar plot to be aggregated by the deviceType. Call the plot function of the Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EperDtDF = spark.sql(\"SELECT ts,deviceType from deviceData\").groupBy(\"deviceType\").count()\n",
    "EperDtDF.cache()\n",
    "EperDtDF.show()\n",
    "\n",
    "EperDtPanda = EperDtDF.toPandas().set_index('deviceType')\n",
    "\n",
    "ax = EperDtPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"deviceType\")\n",
    "ax.set_ylabel(\"events count\")\n",
    "ax.set_title('count of events by deviceType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the breakdown of the devices per device type? \n",
    "The bar chart is plotted in the same way as before, but now we will also show the pie chart of the data. Pandas data frame supports different plot types. Using pie generates a pie chart with percentage sizes of the pieces.\n",
    "To write the actual count of the devices, instead of percentages, use the autopct argument - multiply by the total amount of devices and divide by 100 to get the actual numbers. \n",
    "The total is calculated by using the *sum()* function of Pandas data frame, which sums up the device count of all the deviceTypes. The sum function returns a DataFrame, so use the *[0]* index to get only the value into the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DperDtDF = spark.sql(\"SELECT deviceId,deviceType from deviceData\").groupBy(\"deviceType\").agg(countDistinct('deviceId'))\n",
    "EperDtDF.cache()\n",
    "DperDtDF.show()\n",
    "\n",
    "# bar chart of deviceId by deviceType\n",
    "EperDtPanda = DperDtDF.toPandas().set_index('deviceType')\n",
    "\n",
    "ax = EperDtPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"deviceType\")\n",
    "ax.set_ylabel(\"devices count\")\n",
    "ax.set_title('count of deviceIds by deviceType')\n",
    "\n",
    "\n",
    "# Pie chart of deviceId by deviceType\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(111)\n",
    "total = EperDtPanda.sum()[0]\n",
    "\n",
    "ax = EperDtPanda.plot(kind='pie', ax=ax, figsize=(5,5), legend=False, shadow=True, subplots=True, autopct=lambda p: '{:.0f}'.format(p * total / 100))\n",
    "plt.title(\"count of deviceIds by deviceType\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many reports have been sent by each device? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EperDdf = spark.sql(\"SELECT deviceId,ts from deviceData\").groupBy(\"deviceId\").count()####.sort()########\n",
    "EperDtDF.cache()\n",
    "EperDdf.show()\n",
    "\n",
    "EperDPanda = EperDdf.toPandas().set_index('deviceId')\n",
    "\n",
    "ax = EperDPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"deviceId\")\n",
    "ax.set_ylabel(\"events count\")\n",
    "ax.set_title('count of events by deviceId')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many reports each event type had? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EperEtDF = spark.sql(\"SELECT eventType,ts from deviceData\").groupBy(\"eventType\").count()\n",
    "EperDtDF.cache()\n",
    "EperEtDF.show()\n",
    "\n",
    "EperEtPanda = EperEtDF.toPandas().set_index('eventType')\n",
    "\n",
    "ax = EperEtPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"eventType\")\n",
    "ax.set_ylabel(\"events count\")\n",
    "ax.set_title('count of events by eventType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many devices reported in a given time interval?\n",
    "\n",
    "**User input required**: Replace the year, month and day in the following cell to specify `start` and `end` interval.\n",
    "\n",
    "For example:\n",
    "\n",
    "`start = datetime(2017, 7, 28, 0, 0, 0)\n",
    "end = datetime(2017, 7, 28, 23, 59, 59)`\n",
    "\n",
    "Make sure that the interval contains device events. You can narrow down the time interval as well. Then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the time interval of interest\n",
    "dbDate = dbName.split(\"_\")[3]\n",
    "dbDate = dbDate.split(\"-\")\n",
    "\n",
    "# datetime(year, month, day, hours, minutes, seconds)\n",
    "start = datetime(int(dbDate[0]), int(dbDate[1]), int(dbDate[2]), 0, 0, 0) \n",
    "end = datetime(int(dbDate[0]), int(dbDate[1]), int(dbDate[2]), 23, 59, 59)\n",
    "\n",
    "print(start)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we filter the data by a time interval, then group the resulting dataFrame by *deviceId*, and count the records for each *deviceId*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by time interval\n",
    "deviceMetaData = sparkDf.select('deviceId','deviceType','ts','timestamp','eventType').filter((col('ts')>=start) & (col('ts')<=end))\n",
    "\n",
    "deviceMetaData.cache()\n",
    "#deviceMetaData.show()\n",
    "\n",
    "#how many devices reported in interval\n",
    "byDevice = deviceMetaData.groupby(['deviceId']).count()\n",
    "byDevice.cache()\n",
    "\n",
    "print(\"Number of events by deviceId in the interval: \")\n",
    "byDevice.show()\n",
    "print(\"total number of devices reported in the interval:\", byDevice.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of rows by time span for a specific device, using the filter function of Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byDevice.filter(byDevice[\"deviceId\"]== deviceId).show() ##also show 5 with lowest counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the numeric columns for further analytics; only a subset of the numeric columns are selected for this demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all numeric columns of the DataFrame\n",
    "numericCols = [name_dt for name_dt in sparkDf.dtypes if (('double' in name_dt[1]) or ('int' in name_dt[1]) or ('long' in name_dt[1]))] \n",
    "\n",
    "#numericCols is a list of pairs (columnName, dataType), here we select only the column name into the allkeys list\n",
    "allkeys = [x[0] for x in numericCols]\n",
    "print(\"all numeric columns\", allkeys)\n",
    "\n",
    "#select only 5 numeric columns for further detailed visualizations\n",
    "keys = ['oa','ob','og']\n",
    "print(\"selected 3 numeric columns\", keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device type sensor visualization \n",
    "\n",
    "Summary of sensor data that is reported by all devices of a device type, for example:\n",
    "\n",
    "1. What is the Average/Min/Max of all reported sensor values? \n",
    "2. Can I see a histogram of a sensor's output?   \n",
    "3. What is the correlation between two sensors?\n",
    "\n",
    "#### Average/Min/Max of all reported sensor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "#showing visualization for device type \n",
    "\n",
    "#show summary only for the selected 5 columns, for easier view, since we have too many columns to fit in a row\n",
    "dfKeysType = sparkDf.select(*keys).filter(sparkDf[\"deviceType\"]==deviceType)\n",
    "dfKeysType.cache()\n",
    "\n",
    "dfKeysType.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of a device type sensor's output\n",
    "\n",
    "1.\tUse Spark DataFrame to prepare the histogram for each specific sensor (key) (using rdd.flatMap)\n",
    "2.\tCreate Pandas DataFrame from the calculated histogram with 2 columns: \"bin\" and \"frequency\".\n",
    "3.\tPlot the histogram using Pandas plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    histogram = dfKeysType.select(key).rdd.flatMap(lambda x: x).histogram(11)\n",
    "    \n",
    "    #print histogram\n",
    "    pandaDf = pd.DataFrame(list(zip(list(histogram)[0],list(histogram)[1])),columns=['bin','frequency']).set_index('bin')\n",
    "    ax =pandaDf.plot(kind='bar')\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title('Histogram of ' + key + ' sensor output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between two sensors\n",
    "\n",
    "Correlation between two sensors can be plotted using Pandas plot with kind='scatter'. Remember that *dfKeysType* is a Spark DataFrame that includes only our selected 5 columns and is filtered by *deviceType*. You can also filter by *timestamp* to decrease the amount of data that is output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1=\"oa\"\n",
    "key2=\"ob\"\n",
    "\n",
    "pandaDF = dfKeysType.toPandas()\n",
    "ax = pandaDF.plot(kind='scatter', x=key1, y=key2, s=5, figsize=(7,7))\n",
    "ax.set_title('Relationship between ' + key1 + ' and ' + key2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all the correlations of the selected 5 columns, together with a histogram on a diagonal, use the Pandas scatter_matrix function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(pandaDF, figsize=(18,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix can be plotted, using Pandas corr() function on the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pandaDF.corr()\n",
    "\n",
    "# plot correlation matrix\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,5,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(keys)\n",
    "ax.set_yticklabels(keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor deep dive\n",
    "\n",
    "Sensor deep dive output is similar to the device type sensor visualization but the data is filtered by *deviceId*.\n",
    "\n",
    "#### Average/Min/Max of all reported sensor values by the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing visualization for specific deviceID set above\n",
    "#show summary only for a selected group of columns, for easier view, since we have too many columns to fit in a row\n",
    "dfKeysDev = sparkDf.select(*keys).filter(sparkDf[\"deviceId\"]==deviceId)\n",
    "dfKeysDev.cache()\n",
    "\n",
    "dfKeysDev.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the lower to upper quartile values of the data, with a line at the median. The whiskers extend from the box to show the range of the data. Beyond the whiskers, data are considered outliers and are plotted as individual points.\n",
    "\n",
    "A box plot for each devices sensor, produced with the Pandas plot function with kind=\"box\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "pandaDF.plot(kind='box', subplots=True, layout=(10,3), sharex=False, sharey=False, figsize=(25,60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Histogram of a device's sensor output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    try:\n",
    "        #The histogram is built with spark. Only the groupped by bins data will be converted to Pandas DataFrame\n",
    "        histogram = dfKeysDev.select(key).rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "        #print histogram\n",
    "        pandaDf = pd.DataFrame(zip(list(histogram)[0],list(histogram)[1]),columns=['bin','frequency']).set_index('bin')\n",
    "        ax = pandaDf.plot(kind='bar')\n",
    "        ax.set_ylabel(\"frequency\")\n",
    "        ax.set_title('Histogram of ' + key + ' sensor output')\n",
    "   \n",
    "    except: \n",
    "        print(\"no values for sensor \" + key + \" for device \" + deviceId + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms can also be built more easily with Pandas DataFrame, in case the dfKeysDev DataFrame is not too large. For the case of big data, spark is more scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "pandaDF.hist(layout=(3,3), sharex=False, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density Plots\n",
    "\n",
    "Kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n",
    "\n",
    "**Note**: here we convert the data into Pandas DataFrame, after we filtered by deviceId and selected a subset of keys. In case this is still too much data for the Pandas DataFrame to handle, consider selecting fewer keys and filtering by time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "\n",
    "ax = pandaDF.plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How a specific device sensor value changes over time\n",
    "\n",
    "Maximum, minimum, and average lines are shown on plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "#showing visualization for specific deviceID set above\n",
    "\n",
    "print(keys)\n",
    "for key in keys:\n",
    "    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from deviceData where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n",
    "    df.cache()\n",
    "    if (df.count() > 0):\n",
    "        pandaDF = df.toPandas()\n",
    "        \n",
    "        ax = pandaDF.plot(x='ts', y=key , legend=False, figsize=(15,9), ls='-', marker='o')\n",
    "        ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "        ax.set_title(key + ' over time')\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Draw lines to showcase the upper and lower threshold\n",
    "        ax.axhline(y=pandaDF[key].min(),c=\"red\",linewidth=2,zorder=0)\n",
    "        ax.axhline(y=pandaDF[key].max(),c=\"red\",linewidth=2,zorder=0)\n",
    "        ax.axhline(y=pandaDF[key].mean(),c=\"green\",linewidth=2,zorder=0, ls='--')\n",
    "    \n",
    "        ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Data can be aggregated by intervals, for example by seconds/minutes/hours in Spark and displayed as aggregated plots (average, minimum, maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "from functools import reduce\n",
    "\n",
    "#showing visualization for specific deviceID set above\n",
    "\n",
    "for key in keys:\n",
    "    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from deviceData where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n",
    "\n",
    "    df = df.groupBy(\"deviceId\", window(\"ts\", \"30 seconds\")).agg(max(key), min(key), mean(key))\n",
    "    #df = df.groupBy(\"deviceId\", window(\"ts\", \"1 minute\")).agg(max(key), min(key), mean(key))\n",
    "    #df.groupBy(\"deviceId\", window(\"ts\", \"5 minutes\")).agg(max(key), min(key), mean(key))\n",
    "    #df.groupBy(\"deviceId\", window(\"ts\", \"1 hour\")).agg(max(key), min(key), mean(key))\n",
    "    \n",
    "    #change automatic name of aggregated column\n",
    "    oldColumns = df.schema.names\n",
    "    newColumns = [\"deviceId\", \"window\", \"max\", \"min\", \"avg\"]\n",
    "    df = reduce(lambda df, idx: df.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df)\n",
    "    \n",
    "    win_to_ts =  udf (lambda d: d.start, TimestampType())\n",
    "\n",
    "    df = df.withColumn('ts', win_to_ts(col('window')))\n",
    "    df = df.select('deviceId','ts',\"max\", \"min\", \"avg\")\n",
    "    df.cache()\n",
    "    \n",
    "    if (df.count() > 0):\n",
    "        pandaDF = df.toPandas()\n",
    "\n",
    "        ax = pandaDF.plot(x='ts', y='min', legend=True, figsize=(15,9), ls='-', marker='o', c=\"red\")\n",
    "        ax = pandaDF.plot(ax=ax, x='ts', y='max', legend=True, figsize=(15,9), ls='-', marker='o', c=\"red\")\n",
    "        ax = pandaDF.plot(ax=ax, x='ts', y='avg', legend=True, figsize=(15,9), ls='-', marker='o', c=\"green\")\n",
    "        \n",
    "        #'S' secondly frequency, 'T' minutely frequency, 'H' hourly frequency\n",
    "        xtick = pd.date_range(start=pandaDF['ts'].min(), end=pandaDF['ts'].max(), freq='30S')\n",
    "        ax.set_xticks(xtick)\n",
    "\n",
    "        ax.xaxis_date()\n",
    "        ax.set_title(key + ' over time groupped by 30 sec')\n",
    "        ax.set_ylabel(key)\n",
    "    \n",
    "        ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Big Data is also possible to show the plot around the extremum point. In the example below we show values spanned in 4 seconds around the maximum value. Note that the aggregation is done inside Spark and Pandas is used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "#showing visualization for specific deviceID\n",
    "\n",
    "for key in keys:\n",
    "    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from deviceData where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n",
    "    df.cache()\n",
    "    if (df.count() > 0):\n",
    "        #find max and filter around them\n",
    "        max_value = df.select(max(key)).collect()[0][0]\n",
    "        maxts = df.filter(df[key] == max_value).rdd.map(lambda r: r['ts']).collect()[0]\n",
    "        startts = maxts - pd.DateOffset(seconds=4) #(minutes=2)#(days=15)\n",
    "        endts = maxts + pd.DateOffset(seconds=4)\n",
    "        df_max = df.filter((col('ts')>=startts) & (col('ts')<=endts))\n",
    "\n",
    "        pandaDF = df_max.toPandas()\n",
    "\n",
    "        ax = pandaDF.plot(x='ts', y=key , legend=False, figsize=(15,9), ls='-', marker='o')\n",
    "        ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "        ax.set_title(key + ' over time')\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Draw lines to showcase the upper and lower threshold\n",
    "        ax.axhline(y=pandaDF[key].min(),c=\"red\",linewidth=2,zorder=0)\n",
    "        ax.axhline(y=pandaDF[key].max(),c=\"red\",linewidth=2,zorder=0)\n",
    "        ax.axhline(y=pandaDF[key].mean(),c=\"green\",linewidth=2,zorder=0, ls='--')\n",
    "    \n",
    "        ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare between the sensor values of devices over time\n",
    "\n",
    "The *dfKeysDev* DataFrame contains only keys columns, with no ts column, so we will create a new data frame that will also include the *ts*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing visualization for specific deviceID\n",
    "\n",
    "columns = list(keys)\n",
    "columns.append('ts')\n",
    "df = sparkDf.select(*columns).filter(sparkDf[\"deviceId\"]==deviceId)\n",
    "\n",
    "pandaDF = df.toPandas().set_index('ts')\n",
    "ax = pandaDF.plot(figsize=(15,9),ls='', marker='o')   \n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "ax.set_title(', '.join(keys) + ' over time')\n",
    "ax.grid(True)\n",
    "ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "\n",
    "pd.plotting.scatter_matrix(pandaDF, figsize=(18,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "\n",
    "Anomaly detection will be performed using *z-score*. *z-score* is a standard score that indicates how many standard deviations an element is from the mean. A z-score can be calculated from the following formula:\n",
    "`z = (X - µ) / σ`\n",
    "where z is the *z-score*, *X* is the value of the element, *µ* is the population mean, and *σ* is the standard deviation.\n",
    "\n",
    "A higher *z-score* value represents a larger deviation from the mean value which can be interpreted as abnormal.\n",
    "\n",
    "We will calculate *z-score* for each selected column (sensor) of each device type, and plot only the sensors that have spikes. We define a spike in the following function spike(row), by reported value having z-score above 3 or below -3. Observe that the values for which the *z-score* is above 3 or below -3, are marked as abnormal events in the following graph.\n",
    "\n",
    "**Note**: The code triggers a number of Spark jobs and might take a few seconds to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings if any\n",
    "import warnings\n",
    "from pyspark.sql.functions import mean, min, max, mean, stddev\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "This function detects the spike and dip by returning a non-zero value \n",
    "when the z-score is above 3 (spike) and below -3(dip). Incase if you \n",
    "want to capture the smaller spikes and dips, lower the zscore value from \n",
    "3 to 2 in this function.\n",
    "'''\n",
    "upperThreshold = 3\n",
    "lowerThreshold = -3\n",
    "def spike(row):\n",
    "    if(row['zscore'] >=upperThreshold or row['zscore'] <=lowerThreshold):\n",
    "        return row[key]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#get the list of available devices\n",
    "deviceTypes = sparkDf.select(\"deviceType\").groupBy(\"deviceType\").count().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "#calculate for each device type and each key\n",
    "for devt in deviceTypes:\n",
    "    for key in keys:\n",
    "        df = spark.sql(\"SELECT deviceType, ts,\" + key +\" from deviceData where deviceType='\" + devt + \"'\").where(col(key).isNotNull())\n",
    "        if (df.count() > 0):\n",
    "            pandaDF = df.toPandas().set_index(\"ts\")\n",
    "            \n",
    "            # calculate z-score and populate a new column\n",
    "            pandaDF['zscore'] = (pandaDF[key] - pandaDF[key].mean())/pandaDF[key].std(ddof=0)\n",
    "\n",
    "            #add new column - spike, and calculate its value based on the thresholds, usinf spike function, defined above\n",
    "            pandaDF['spike'] = pandaDF.apply(spike, axis=1)\n",
    "            \n",
    "            \n",
    "            #plot the chart, only if spikes were detected (not all values of \"spike\" are zero)\n",
    "            if (pandaDF['spike'].nunique() > 1):\n",
    "                # select rows that are required for plotting\n",
    "                plotDF = pandaDF[[key,'spike']]\n",
    "                #calculate the y minimum value\n",
    "                y_min = (pandaDF[key].max() - pandaDF[key].min()) / 10\n",
    "                fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "                ax.set_ylim(plotDF[key].min() - y_min, plotDF[key].max() + y_min)\n",
    "                x_filt = plotDF.index[plotDF.spike != 0]\n",
    "                plotDF['spikes'] = plotDF[key]\n",
    "                y_filt = plotDF.spikes[plotDF.spike != 0]\n",
    "                #Plot the raw data in blue colour\n",
    "                line1 = ax.plot(plotDF.index, plotDF[key], '-', color='blue', animated = True, linewidth=1, marker='o')\n",
    "                #plot the anomalies in red circle\n",
    "                line2 = ax.plot(x_filt, y_filt, 'ro', color='red', linewidth=2, animated = True)\n",
    "                #Fill the raw area\n",
    "                ax.fill_between(plotDF.index, (pandaDF[key].min() - y_min), plotDF[key], interpolate=True, color='blue',alpha=0.6)\n",
    "\n",
    "                # calculate the sensor value that is corresponding to z-score that defines a spike\n",
    "                valUpperThreshold = (pandaDF[key].std(ddof=0) * upperThreshold) + pandaDF[key].mean()\n",
    "                # calculate the sensor value that is corresponding to z-score that defines a dip\n",
    "                valLowerThreshold = (pandaDF[key].std(ddof=0) * lowerThreshold) + pandaDF[key].mean()\n",
    "\n",
    "                #plot the thresholds\n",
    "                ax.axhline(y=valUpperThreshold,c=\"red\",linewidth=2,zorder=0,linestyle='dashed',label='Upper threshold')\n",
    "                ax.axhline(y=valLowerThreshold,c=\"red\",linewidth=2,zorder=0,linestyle='dotted',label='Lower threshold')\n",
    "                \n",
    "                # Label the axis\n",
    "                ax.set_xlabel(\"Sequence\",fontsize=20)\n",
    "                ax.set_ylabel(key,fontsize=20)\n",
    "                ax.set_title(\"deviceType: \" + devt + \" sensor:\" + key)\n",
    "                plt.tight_layout()\n",
    "                plt.legend()\n",
    "                \n",
    "                print(\"Device Type: \" + devt + \", sensor: \" + key)\n",
    "                print(\"Upper treshould based on the z-score calculation: \" , upperThreshold , \": \" , valUpperThreshold)\n",
    "                print(\"Lower treshould based on the z-score calculation: \", lowerThreshold, \": \" , valLowerThreshold)\n",
    "                \n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The red marks indicate unexpected spikes where the z-score value is greater than 3 or less than -3. To detect lower spikes, modify the value to 2 or less. Similarly, if you want to detect only the higher spikes, increase the z-score value from 3 to 4 or more.\n",
    "\n",
    "For complete solution tutorial, refer [Gather, Visualize and Analyze IoT data](http://console.bluemix.net/docs/tutorials/gather-visualize-analyze-iot-data.html#gather-visualize-and-analyze-iot-data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
